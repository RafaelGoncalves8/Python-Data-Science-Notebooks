
# coding: utf-8

# # Index

# 1. Machine Learning
#     - Linear Regression
#     - Example of Multivariate Linear Regression
#     - Logistic Regression
#     - Example of One-vs-all Logistic Regression
# 2

# # Notation

# $a$ - 1D variable  
# $A$ - 1D constant  
# $\mathbf{a}$ - 2D Column vector (nx1)  
# $\mathbf{A}$ - 2D Matrix  

# $\mathbf{a}^T$ - Row vector (1xn)  
# $\mathbf{A}^T$ - Transpose matrix  
# $\mathbf{A}^{-1}$ - Inverse matrix  
# $\mathbf{A}^n$ - nth power of A (A\*A\*...A n times)  
# $\mathbf{A^n}$ - Elementwise nth power of A

# $\mathbf{0_{n}}$ - n-columns vector of zeros  
# $\mathbf{1_{n}}$ - n-columns vector of ones

# $\mathbf{0_{mxn}}$ - mxn matrix of zeros  
# $\mathbf{1_{mxn}}$ - mxn matrix of ones  
# $\mathbf{I_n} = \mathbf{I_{nxn}}$ - nxn identity matrix (eye)

# $\mathbf{A}\cdot\mathbf{B}=\mathbf{A}\mathbf{B}$ - Inner product  
# $\mathbf{A}\times\mathbf{B}$ - Outer product  
# $\mathbf{A} * \mathbf{B}$ - Convolution  
# $\mathbf{A} \odot \mathbf{B}$ - Elementwise multiplication (Hadamard product)

# $\mathbf{A} / \mathbf{B} = \frac{\mathbf{A}}{\mathbf{B}}$ - Division  
# 
# $\mathbf{A} \oslash \mathbf{B}$ - Elementwise division

# In[ ]:




